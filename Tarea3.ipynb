{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed0d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum \n",
    "import tqdm \n",
    "from typing import Generator, Dict, List, Tuple \n",
    "import pickle \n",
    "import math \n",
    "import random \n",
    "from gensim.parsing.preprocessing import remove_stopword_tokens \n",
    "from gensim.models import Phrases \n",
    "from gensim.models.phrases import Phraser \n",
    "from gensim.utils import simple_preprocess \n",
    "from gensim.corpora import Dictionary \n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords \n",
    "import spacy \n",
    "from spacy.lang. en import English \n",
    "from datetime import datetime \n",
    "from collections import defaultdict \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e9f580",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30376/858116657.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_trf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mPOS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'POS'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mNEG\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NEG'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blank:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \"\"\"\n\u001b[0;32m    452\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\en_core_web_trf\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m     return load_model_from_path(\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m     nlp = load_model_from_config(\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;31m# registry, including custom subclasses provided via entry points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mlang_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lang\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     nlp = lang_cls.from_config(\n\u001b[0m\u001b[0;32m    529\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, vocab, disable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1807\u001b[0m                     \u001b[1;31m# The pipe name (key in the config) here is the unique name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m                     \u001b[1;31m# of the component, not necessarily the factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1809\u001b[1;33m                     nlp.add_pipe(\n\u001b[0m\u001b[0;32m   1810\u001b[0m                         \u001b[0mfactory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m                         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipe_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    793\u001b[0m                     \u001b[0mlang_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                 )\n\u001b[1;32m--> 795\u001b[1;33m             pipe_component = self.create_pipe(\n\u001b[0m\u001b[0;32m    796\u001b[0m                 \u001b[0mfactory_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    653\u001b[0m                 \u001b[0mlang_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m             )\n\u001b[1;32m--> 655\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m         \u001b[0mpipe_meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_factory_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# This is unideal, but the alternative would mean you always need to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E002] Can't find factory for 'transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_trf')\n",
    "class sentiments(enum.Enum):\n",
    "    POS='POS'\n",
    "    NEG='NEG'\n",
    "\n",
    "def split_data(data: List,weights: List -(0.7,0.15,0.15)):\n",
    "    split={\n",
    "        'train':[],\n",
    "        'test':[],\n",
    "        'validation':[],\n",
    "    }\n",
    "    for word in data:\n",
    "        subset = random.choices(['train','test','validation'],weights=weights)[0]\n",
    "        split[subset].append(word)\n",
    "\n",
    "    return split\n",
    "def sentences_to_words(sentences: List[str]) -> List[List[str]]:\n",
    "    words =[]\n",
    "    pbar=tqdm.tqdm(range(len(sentences)))\n",
    "    pbar.set_description('Sentences to words')\n",
    "    for i in pbar:\n",
    "        words.append(simple_reprocess(str(sentences[i]),deacc=True))\n",
    "    return words\n",
    "def remoce_stopwords(documents: List[List[str]]) -> List[List[str]]:\n",
    "    pbar = tqdm.tqdm(range(len(documents)))\n",
    "    pbar.set_description('Remove StopWords')\n",
    "    docs=[]\n",
    "    for i in pbar:\n",
    "        docs.append(remove_stopword_tokens(documents[i],stopwords=stopwords.words('english')))\n",
    "    \n",
    "    return docs\n",
    "def create_ngrams(ngram_model_lst, documents: List[List[str]]):\n",
    "    pbar = tqdm.tqdm(range(len(ngram_model_lst)))\n",
    "    pbar.set_description('Create N-grams')\n",
    "    for i in pbar:\n",
    "        documents=[ngram_model_lst[i][doc] for doc in documents]\n",
    "    return documents\n",
    "def learn_ngrams(n,min_c, th, documents: List[List[str]]) -> List[List[str]]:\n",
    "    print('Learning N-grams')\n",
    "    ngram_model_lst = []\n",
    "    ngram = Phrases(documents, min_count= min_c, threshold = th )\n",
    "    ngram_mod = Phraser(ngram)\n",
    "    ngram_model_lst.append(ngram_mod)\n",
    "    if n >2:\n",
    "        for i in range(3,n+1):\n",
    "            documents = [ngram_model_lst[i-3][doc]for doc in documents]\n",
    "            ngram_model_lst.append(Phraser(Phrases(documents, min_count = min_c, threshold = th)))\n",
    "    return ngram_model_lst\n",
    "\n",
    "def lemma(nlp: English, texts: List[List[str]], allowed_postags: List = None) -> List[List[str]]:\n",
    "    if allowed_postags is None:\n",
    "        allowed_postags = ['NOUN','ADJ','VERB','ADV']\n",
    "        texts_out = []\n",
    "        pbar = tqdm.tqdm(range(len(texts)))\n",
    "        pbar.set_description('Lemmatization')\n",
    "        for i in pbar:\n",
    "            doc = nlp(\" \".join(texts[i]))\n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def tokenize(stopwords, lemmatization, ngrams, documents: List[str], ngram_model_lst) -> List[List[str]]:\n",
    "    document_words = sentences_to_words(documents)\n",
    "    if lemmatization:\n",
    "        document_words = lemma(nlp, document_words)\n",
    "    if stopwords:\n",
    "        document_words = remove_stopwords(document_words)\n",
    "    if ngrams:\n",
    "        document_words = create_ngrams(ngram_model_lst, document_words)\n",
    "    return document_words\n",
    "\n",
    "def train_tokenize(stopwords, lemmatization,documents: List[str]) -> List[List[str]]:\n",
    "    document_words = sentences_to_words(documents)\n",
    "    if lemmatization:\n",
    "        document_words= lemma(nlp,document_words)\n",
    "    if stopwords:\n",
    "        document_words=remove_stopwords(document_words)\n",
    "        \n",
    "    return document_words\n",
    "\n",
    "def create_dictionary(documents: List[List[str]]):\n",
    "    return Dictionary(documents)\n",
    "\n",
    "class NB_sentiment_analysis():\n",
    "    def _init_(self,**kwargs):\n",
    "        self.stopwords=kwargs.get('stopwords',True)\n",
    "        self.ngrams = kwargs.get('ngrams',True)\n",
    "        self.n = kwargs.get('n',2)\n",
    "        self.ngram_mincount = kwargs.get('ngram_min_count',5)\n",
    "        self.ngram_th = kwargs.get('ngram_th',10)\n",
    "        self.lemmatization = kwargs.get('lemmatization',True)\n",
    "        self.ngram_model_lst=[]\n",
    "        self.is_trained= False\n",
    "    \n",
    "    def fit(self, train_POS,train_NEG,verbose=False):\n",
    "        print(datetime.now())\n",
    "        print('Negative class tokenization')\n",
    "        tokenized_NEG = train_tokenize(self.stopwords, self.lemmatization, train_NEG)\n",
    "        print('Positive class Tokenization')\n",
    "        tokenized_POS = train_tokenize(self.stopwords,self.lemmatization, train_POS)\n",
    "        \n",
    "        if self.ngrams:\n",
    "            self.ngram_model_lst = learn_ngrams(self.n,self.ngram_mincount, self.ngram_th, tokenized_POS+tokenized_NEG)\n",
    "            print('Negative class N-grams')\n",
    "            tokenized_NEG =create_ngrams(self.ngram_model_lst,tokenized_NEG)\n",
    "            print('Positive class N-grams')\n",
    "            tokenized_POS = create_ngrams(self.ngram_model_lst,tokenized_POS)\n",
    "            \n",
    "        positive_words = [item for sublist in tokenized_POS for item in sublist]\n",
    "        negative_words = [item for sublist in tokenized_POS for item in sublist]\n",
    "        self.dictionary = create_dictionary([negative_words,positive_words])\n",
    "        positive_bow = self.dictionary.doc2bow(positive_words)\n",
    "        negative_bow = self.dictionary.doc2bow(negative_bow)\n",
    "        total_negative_words = len(negative_words)+ len(self.dictionary)\n",
    "        total_positive_words = len(positive_words)+ len(self.dictionary)\n",
    "        self.negative_word_probs={}\n",
    "        for id, count in negative_bow:\n",
    "            self.negative_word_probs[self.dictionary[id]] = {\n",
    "                'id': id,\n",
    "                'logprob': np.log((count+1)/total_negative_words),\n",
    "            }\n",
    "        self.negative_word_probs = defaultdict(lambda: {'id': -1, 'logprob': np.log(1/total_negative_words)}, self.negative_word_probs)\n",
    "        self.positive_word_probs={}\n",
    "        \n",
    "        for id, count in positive_bow:\n",
    "            self.positive_word_probs[self.dictionary[id]] = {\n",
    "                'id': id,\n",
    "                'logprob': np.log((count+1)/total_positive_words),\n",
    "            }\n",
    "        self.positive_word_probs = defaultdict(lambda: {'id': -1, 'logprob': np.log(1/total_positive_words)}, self.positive_word_probs)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.negative_prob = np.log(len(negative_words)/(len(negative_words)+len(positive_words)))\n",
    "        self.positive_prob = np.log(len(positive_words)/(len(negative_words)+len(positive_words)))\n",
    "        self.is_trained=True\n",
    "        print(datetime.now())\n",
    "        \n",
    "    def predict(self,document):\n",
    "        doc = [document]\n",
    "        tokenized_doc = tokenize(self.stopwords,self.lemmatization,self.ngrams,doc,self.ngram_model_lst)\n",
    "        pos_prob = self.positive_prob\n",
    "        neg_prob = self.negative_prob\n",
    "        for token in tokenized_doc[0]:\n",
    "            pos_prob+= self.positive_word_probs[token]['logprob']\n",
    "            neg_prob+= self.positive_word_probs[token]['logprob']\n",
    "        \n",
    "        if pos_prob > neg_prob:\n",
    "            sentiment = Sentiments.POS\n",
    "        else:\n",
    "            sentiment = Sentiments.NEG\n",
    "            \n",
    "        return sentiment\n",
    "    \n",
    "    def val_test(self,pos,neg):\n",
    "        tokenized_POS = tokenize(self.stopwords,self.lemmatization,self.ngrams, pos,self.ngram_model_lst)\n",
    "        tokenized_NEG = tokenize(self.stopwords,self.lemmatization,self.ngrams, neg,self.ngram_model_lst)\n",
    "        tp=0\n",
    "        fp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        pos_prob=self.positive_prob\n",
    "        neg_prob=self.negative_prob\n",
    "        pbar=tqdm.tqdm(range(len(tokenized_POS)))\n",
    "        pbar.set_description('Positive Validation/Test')\n",
    "        for i in pbar:\n",
    "            for token in tokenized_POS[i]:\n",
    "                pos_prob+=self.positive_word_probs[token]['logprob']\n",
    "                neg_prob+=self.positive_word_probs[token]['logprob']\n",
    "            if pos_prob > neg_prob:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        pbar=tqdm.tqdm(range(len(tokenized_NEG)))\n",
    "        pbar.set_description('Negative Validation/Test')\n",
    "        for i in pbar:\n",
    "            for token in tokenized_NEG[i]:\n",
    "                pos_prob+=self.positive_word_probs[token]['logprob']\n",
    "                neg_prob+=self.positive_word_probs[token]['logprob']\n",
    "            if pos_prob > neg_prob:\n",
    "                fp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1= 2*precision*recall/(precision+recall)\n",
    "        fpr= fp/(fp+tn)\n",
    "        fnr = fn/(tp+fn)\n",
    "        self.metrics={'accuracy':acc,'precision':precision,'recall':recall,'F1':f1,'False Positive Rate': fpr,'False Negative Rate':fnr}\n",
    "        return tp,tn,fp,fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81c2dc",
   "metadata": {},
   "source": [
    "Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d044a90e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30376/3636687071.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSentiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mSentiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNEG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreview_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSentiments\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpositive_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'POS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('review.csv')\n",
    "df['rating'] = df['rating'].astype(dtype='int64')\n",
    "df['sentiment']=df['rating'].apply(lambda x: Sentiments.POS if x >= 30 else Sentiments.NEG)\n",
    "review_classes = {sentiment.value: df[df['sentiment']== sentiment]['review'].values.tolist() for sentiment in Sentiments}\n",
    "positive_reviews = review_classes['POS']\n",
    "negative_reviews = review_classes['NEG']\n",
    "split_neg = split_data(negative_reviews)\n",
    "split_pos = split_data(positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96378e9",
   "metadata": {},
   "source": [
    "Entrenamiento de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc43197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NB_sentiment_analysis()\n",
    "model.fit(split_pos['train'],split_neg['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a54aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.val_test(split_pos['validation'],split_neg['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3daac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ada980",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Good location on the NW of town, especially if youre going to Antigua spanish school. 100q for good sized private'\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews[100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
